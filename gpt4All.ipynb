{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9f08c94-d836-442e-8e37-cfbf7f64ade7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Llamamos a la libreria gpt4all la cual es compatible con Python 3.8 o superior\n",
    "## Se instala con pip install gpt4all\n",
    "import gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91a4e866-31ec-4cbd-bee6-1964a2e929fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to retrieve model",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## Iniciamos el modelo, si bien lo podemos hacer con Pytouch o Tensorflow, gpt4all ya trae todo lo requerido para trabajar\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m## Al definir el parametro device podremos indicar que use la GPU de nVidia, pero si tenemos una intel o AMD solo escribiremos intel o AMD en vez de GPU\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m## El modelo a usar sera: mistral-7b-instruct-v0.1.Q4_0.gguf y debera estar en la misma carpeta del notebook o bien deberemos escribir el path completo a donde se encuentre\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m## Ejemplo: c:\\\\Modelo\\mistral-7b-instruct-v0.1.Q4_0.gguf\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m gpt4all\u001b[38;5;241m.\u001b[39mGPT4All(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistral-7b-instruct-v0.1.Q4_0.gguf\u001b[39m\u001b[38;5;124m\"\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m'\u001b[39m, allow_download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\gpt4all\\Lib\\site-packages\\gpt4all\\gpt4all.py:101\u001b[0m, in \u001b[0;36mGPT4All.__init__\u001b[1;34m(self, model_name, model_path, model_type, allow_download, n_threads, device, n_ctx, ngl, verbose)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type \u001b[38;5;241m=\u001b[39m model_type\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Retrieve model and download if allowed\u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig: ConfigType \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve_model(model_name, model_path\u001b[38;5;241m=\u001b[39mmodel_path, allow_download\u001b[38;5;241m=\u001b[39mallow_download, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m _pyllmodel\u001b[38;5;241m.\u001b[39mLLModel(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m], n_ctx, ngl)\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m device \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\.conda\\envs\\gpt4all\\Lib\\site-packages\\gpt4all\\gpt4all.py:193\u001b[0m, in \u001b[0;36mGPT4All.retrieve_model\u001b[1;34m(model_name, model_path, allow_download, verbose)\u001b[0m\n\u001b[0;32m    191\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m GPT4All\u001b[38;5;241m.\u001b[39mdownload_model(model_filename, model_path, verbose\u001b[38;5;241m=\u001b[39mverbose, url\u001b[38;5;241m=\u001b[39murl)\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to retrieve model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m config\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to retrieve model"
     ]
    }
   ],
   "source": [
    "## Iniciamos el modelo, si bien lo podemos hacer con Pytouch o Tensorflow, gpt4all ya trae todo lo requerido para trabajar\n",
    "## Al definir el parametro device podremos indicar que use la GPU de nVidia, pero si tenemos una intel o AMD solo escribiremos intel o AMD en vez de GPU\n",
    "## El modelo a usar sera: mistral-7b-instruct-v0.1.Q4_0.gguf y debera estar en la misma carpeta del notebook o bien deberemos escribir el path completo a donde se encuentre\n",
    "## Ejemplo: c:\\\\Modelo\\mistral-7b-instruct-v0.1.Q4_0.gguf\n",
    "\n",
    "model = gpt4all.GPT4All(\"mistral-7b-instruct-v0.1.Q4_0.gguf\", device='gpu', allow_download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cd468e1-607f-4213-9ca4-6900bea9d462",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Definimos las variables \n",
    "\n",
    "prompt_template = \"\"\n",
    "system_template = \"\"\n",
    "pre_chat = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b81dd0c-8b99-4634-8d66-e3996176b078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default prompt template: '### Human: \\n{0}\\n### Assistant:\\n'\n"
     ]
    }
   ],
   "source": [
    "## Cargamos el template del modelo, si este no tiene uno se fuerza uno\n",
    "\n",
    "if (repr(model.config['promptTemplate']) == \"\"):\n",
    "    prompt_template = '### Human: \\n{0}\\n### Assistant:\\n'\n",
    "else:\n",
    "    print(\"default prompt template:\", repr(model.config['promptTemplate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24e2886e-8608-4f31-bd55-f88a597924fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Se describe la forma en la que debera responder el modelo\n",
    "system_template = 'Actuaras como asistente telefonico en español.'\n",
    "\n",
    "## Se cagra una muestra de charla, este paso no es importante excepto que querramos forzar un tipo de respuesta\n",
    "mi_array = [\n",
    "    (\"user\", \"Hola\"),\n",
    "    (\"system\", \"Con quien tengo el gusto de hablar\"),\n",
    "    (\"user\", \"Soy Fernando\"),\n",
    "    (\"system\", \"Como estas Fernando, ¿en que te puedo ayudarte?\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03695506-150d-45d1-9c2b-8b658e9d8c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Bienvenido al servicio de asistencia telefónica! ¿Cómo puedo ayudarte hoy?\n"
     ]
    }
   ],
   "source": [
    "# Iniciamos el modelo en modo chat, esta no es la unica dorma de hacerlo, pero si la que nos permite mantener contexto\n",
    "\n",
    "with model.chat_session(system_template, prompt_template):\n",
    "    \n",
    "    ## Recorremos el array para crear un texto previo de como queremos que el sistema responda:\n",
    "    for destino, texto in mi_array:\n",
    "        if (destino == \"user\"):\n",
    "            pre_chat = pre_chat + \"### Human: \\n\" + texto + \"\\n\"\n",
    "        if (destino == \"system\"):\n",
    "            pre_chat = pre_chat + \"### Assistant: \\n\" + texto\n",
    "\n",
    "    ## Parametros opcionales ------------------------------------------------------------------------------------------------------------------\n",
    "    ## prompt (str): El texto de inicio para completar el modelo.\n",
    "    ## max_tokens (int, default: 200): El número máximo de tokens a generar.\n",
    "    ## temp (float, default: 0.7): La temperatura del modelo. Valores más altos aumentan la creatividad pero disminuyen la veracidad.\n",
    "    ## top_k (int, default: 40): Muestrea aleatoriamente entre los tokens más probables según el valor de top_k en cada paso de generación. Establecerlo en 1 para decodificación voraz.\n",
    "    ## top_p (float, default: 0.4): Muestrea aleatoriamente en cada paso de generación entre los tokens más probables cuyas probabilidades suman hasta top_p.\n",
    "    ## min_p (float, default: 0.0): Muestrea aleatoriamente en cada paso de generación entre los tokens más probables cuyas probabilidades son al menos min_p.\n",
    "    ## repeat_penalty (float, default: 1.18): Penaliza al modelo por repetición. Valores más altos reducen la repetición.\n",
    "    ## repeat_last_n (int, default: 64): Cuántos pasos atrás en el historial de generación del modelo se aplica la penalización por repetición.\n",
    "    ## n_batch (int, default: 8): Número de tokens de inicio procesados en paralelo. Valores más altos disminuyen la latencia pero aumentan los recursos requeridos.\n",
    "    ## n_predict (Optional[int], default: None): Equivalente a max_tokens, existe por compatibilidad hacia atrás.\n",
    "    ## streaming (bool, default: False): Si es True, este método devolverá un generador que produce tokens a medida que el modelo los genera.\n",
    "    ## callback (ResponseCallbackType, default: empty_response_callback): Una función con argumentos token_id:int y response:str, que recibe los tokens del modelo a medida que se generan y detiene la generación al devolver False.\n",
    "    ## ------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    ## Enviamos nuestra charla de muestra\n",
    "    model.generate(pre_chat, max_tokens=100, temp=0.5, top_k=40, top_p=0.4, repeat_penalty=1.18, repeat_last_n=64, n_batch=8)\n",
    "\n",
    "    ## Hacemos la primer pregunta\n",
    "    response1 = model.generate(\"Hola\", max_tokens=100, temp=0.5, top_k=40, top_p=0.4, repeat_penalty=1.18, repeat_last_n=64, n_batch=8)\n",
    "    print(response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d8273b-48b3-44af-9a35-0cde5822fc1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

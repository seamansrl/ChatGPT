{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9f08c94-d836-442e-8e37-cfbf7f64ade7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Llamamos a la libreria gpt4all la cual es compatible con Python 3.8 o superior\n",
    "## Se instala con pip install gpt4all\n",
    "import gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91a4e866-31ec-4cbd-bee6-1964a2e929fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Iniciamos el modelo, si bien lo podemos hacer con Pytouch o Tensorflow, gpt4all ya trae todo lo requerido para trabajar\n",
    "## Al definir el parametro device podremos indicar que use la GPU de nVidia, pero si tenemos una intel o AMD solo escribiremos intel o AMD en vez de GPU\n",
    "## El modelo a usar sera: mistral-7b-instruct-v0.1.Q4_0.gguf y debera estar en la misma carpeta del notebook o bien deberemos escribir el path completo a donde se encuentre\n",
    "## Ejemplo: c:\\\\Modelo\\mistral-7b-instruct-v0.1.Q4_0.gguf\n",
    "\n",
    "model = gpt4all.GPT4All(\"mistral-7b-instruct-v0.1.Q4_0.gguf\", device='gpu', allow_download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cd468e1-607f-4213-9ca4-6900bea9d462",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Definimos las variables \n",
    "\n",
    "prompt_template = \"\"\n",
    "system_template = \"\"\n",
    "pre_chat = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b81dd0c-8b99-4634-8d66-e3996176b078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default prompt template: '### Human: \\n{0}\\n### Assistant:\\n'\n"
     ]
    }
   ],
   "source": [
    "## Cargamos el template del modelo, si este no tiene uno se fuerza uno\n",
    "\n",
    "if (repr(model.config['promptTemplate']) == \"\"):\n",
    "    prompt_template = '### Human: \\n{0}\\n### Assistant:\\n'\n",
    "else:\n",
    "    print(\"default prompt template:\", repr(model.config['promptTemplate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24e2886e-8608-4f31-bd55-f88a597924fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Se describe la forma en la que debera responder el modelo\n",
    "system_template = 'Actuaras como asistente telefonico en español.'\n",
    "\n",
    "## Se cagra una muestra de charla, este paso no es importante excepto que querramos forzar un tipo de respuesta\n",
    "mi_array = [\n",
    "    (\"user\", \"Hola\"),\n",
    "    (\"system\", \"Con quien tengo el gusto de hablar\"),\n",
    "    (\"user\", \"Soy Fernando\"),\n",
    "    (\"system\", \"Como estas Fernando, ¿en que te puedo ayudarte?\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03695506-150d-45d1-9c2b-8b658e9d8c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Con quien tengo el gusto de hablar\n"
     ]
    }
   ],
   "source": [
    "# Iniciamos el modelo en modo chat, esta no es la unica dorma de hacerlo, pero si la que nos permite mantener contexto\n",
    "\n",
    "with model.chat_session(system_template, prompt_template):\n",
    "    \n",
    "    ## Recorremos el array para crear un texto previo de como queremos que el sistema responda:\n",
    "    for destino, texto in mi_array:\n",
    "        if (destino == \"user\"):\n",
    "            pre_chat = pre_chat + \"### Human: \\n\" + texto + \"\\n\"\n",
    "        if (destino == \"system\"):\n",
    "            pre_chat = pre_chat + \"### Assistant: \\n\" + texto\n",
    "\n",
    "    ## Parametros opcionales ------------------------------------------------------------------------------------------------------------------\n",
    "    ## prompt (str): El texto de inicio para completar el modelo.\n",
    "    ## max_tokens (int, default: 200): El número máximo de tokens a generar.\n",
    "    ## temp (float, default: 0.7): La temperatura del modelo. Valores más altos aumentan la creatividad pero disminuyen la veracidad.\n",
    "    ## top_k (int, default: 40): Muestrea aleatoriamente entre los tokens más probables según el valor de top_k en cada paso de generación. Establecerlo en 1 para decodificación voraz.\n",
    "    ## top_p (float, default: 0.4): Muestrea aleatoriamente en cada paso de generación entre los tokens más probables cuyas probabilidades suman hasta top_p.\n",
    "    ## min_p (float, default: 0.0): Muestrea aleatoriamente en cada paso de generación entre los tokens más probables cuyas probabilidades son al menos min_p.\n",
    "    ## repeat_penalty (float, default: 1.18): Penaliza al modelo por repetición. Valores más altos reducen la repetición.\n",
    "    ## repeat_last_n (int, default: 64): Cuántos pasos atrás en el historial de generación del modelo se aplica la penalización por repetición.\n",
    "    ## n_batch (int, default: 8): Número de tokens de inicio procesados en paralelo. Valores más altos disminuyen la latencia pero aumentan los recursos requeridos.\n",
    "    ## n_predict (Optional[int], default: None): Equivalente a max_tokens, existe por compatibilidad hacia atrás.\n",
    "    ## streaming (bool, default: False): Si es True, este método devolverá un generador que produce tokens a medida que el modelo los genera.\n",
    "    ## callback (ResponseCallbackType, default: empty_response_callback): Una función con argumentos token_id:int y response:str, que recibe los tokens del modelo a medida que se generan y detiene la generación al devolver False.\n",
    "    ## ------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    ## Enviamos nuestra charla de muestra\n",
    "    model.generate(pre_chat, max_tokens=100, temp=0.5, top_k=40, top_p=0.4, repeat_penalty=1.18, repeat_last_n=64, n_batch=8)\n",
    "\n",
    "    ## Hacemos la primer pregunta\n",
    "    response1 = model.generate(\"Hola\", max_tokens=100, temp=0.5, top_k=40, top_p=0.4, repeat_penalty=1.18, repeat_last_n=64, n_batch=8)\n",
    "    print(response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d8273b-48b3-44af-9a35-0cde5822fc1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
